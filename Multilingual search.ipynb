{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Search\n",
    "\n",
    "\n",
    "**Using Multilingual embedding model search is performed to answer questions in any language.**\n",
    "\n",
    "\n",
    "**Flow of multilingual search:**\n",
    "\n",
    "**1. Data preparation: Loading and chuncking data**\n",
    "\n",
    "**2. Embeddings: EMbed data using multilingual embedding model**\n",
    "\n",
    "**3. Vector store: Add data to FAISS vector store**\n",
    "\n",
    "**4. Retrieval: Extract relevant documents from vetor store for query in any language (depending on embedding model supporting languages)**\n",
    "\n",
    "**5. Answer generation: Generate answer from retrieved context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PoojaT.WINJITBIOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and chunk PDF data\n",
    "\n",
    "**The data loaded is in english language.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of chunks:  45\n"
     ]
    }
   ],
   "source": [
    "# loading pdf data and chunking\n",
    "\n",
    "pdf_data = PDFPlumberLoader(r\"D:\\Winjit_training\\R&D\\MM-LLM\\2306.13549v2.pdf\").load()\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "data_splits = splitter.split_documents(pdf_data)\n",
    "print(\"no of chunks: \", len(data_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PoojaT.WINJITBIOS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# multilingual huggingface embedding model\n",
    "model_name = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={\"device\": \"cpu\", \"trust_remote_code\": True},\n",
    "                encode_kwargs={\"normalize_embeddings\": True},\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add documents to vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27012f16-0c59-43f9-8ff5-78672d1c432f',\n",
       " '52d24163-aadb-4ed2-8197-dc656841ef6f',\n",
       " 'e328a6a7-2ac2-4576-adca-041b08a34bf9',\n",
       " '0a5fcd1d-9079-4680-b600-773e246ed1aa',\n",
       " '9d8a1d88-6eba-4ddd-964b-97609135b122',\n",
       " 'ff1f5b98-1f8d-49ee-958d-b921977ae42a',\n",
       " '4e9971bb-d9e2-4a4c-b615-4c0256bd7580',\n",
       " 'ce8afadd-2c31-43c2-aa0c-a6bf160ccb39',\n",
       " '30c218b8-b1d2-47a6-bdb3-f193bee8dc3c',\n",
       " '1fd38ade-3694-4089-8deb-416c12345228',\n",
       " 'db19986c-b1f5-47f0-b6fb-e46fdfdbaa77',\n",
       " '8b10a599-fae9-429f-b2dd-a6c77a727f69',\n",
       " 'eb643dbd-cee6-47e5-b787-04c6d3223940',\n",
       " '32d6fd38-52a6-4822-897a-33aa79f1f8bb',\n",
       " '92ea899b-9e40-4561-911e-67d9c789bc50',\n",
       " 'd1888dfc-f801-4b1c-8804-0e3f26a4ddb5',\n",
       " 'a088019b-fd40-42bb-a2bd-e2792aa68cc5',\n",
       " '33f95767-6537-44a4-90c0-470fa7eccdcb',\n",
       " '371ab4d6-6cf8-4170-adcd-6744aedb8d7f',\n",
       " '43a0d842-c9b7-4df5-968c-8a499b7cacb9',\n",
       " '62fa7568-2e3f-482e-a00a-8b64e709867d',\n",
       " '2512d7be-41d4-4440-bf71-359d5f6d86a2',\n",
       " '4b1f9a9c-6055-4d97-a4ce-f9c20dff48c2',\n",
       " '6db03560-e4a7-4be8-bfbb-e7a12249a613',\n",
       " '09a5cb60-407f-48be-9436-27ef7db2f11a',\n",
       " 'c16a6ae9-c5d4-4b4e-8232-68393515494e',\n",
       " '54e907e4-a6e9-451f-829f-b70b6c739c06',\n",
       " 'b9a73d7d-40a3-433f-811c-3ed474915a24',\n",
       " '350d1ae6-f185-4f38-ae69-6211b968d5ce',\n",
       " 'f6e00a30-5a08-4e7f-90fa-ed90902ae891',\n",
       " '6cd9dfeb-49c7-47c0-958a-a0252b0f9897',\n",
       " 'b8de7e1e-314f-4c1a-99d0-50440cc6055e',\n",
       " 'bf243025-1749-41d8-811a-19441b54b198',\n",
       " 'a82cd8d2-7508-4281-919d-8c0117035ad4',\n",
       " '5f0ec8f1-de9f-4a57-be63-65f65fb477f6',\n",
       " '4aca5917-b37f-4dc1-9ed5-bb406566177f',\n",
       " 'fb724c62-0972-4526-a11c-02b052424045',\n",
       " 'e5d6c6ec-9a65-4bc8-8ae2-951220dc9835',\n",
       " '59f434d1-283f-4335-b5a9-eee4c36d8f26',\n",
       " '86b97f2c-9f97-4ae9-ad39-df191325e64d',\n",
       " 'ae816232-9618-42a9-9a58-a52501363a6f',\n",
       " 'dcc6770c-a191-4bff-b817-d798b26022e6',\n",
       " 'f93313dd-c98f-41a1-b5b8-eb799daf6ec6',\n",
       " 'd1dc67b8-1c96-4b0c-9af2-313efe4b97c4',\n",
       " '2269032c-d22d-4364-8e02-de6002e46635']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faiss vectorstore\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=faiss.IndexFlatL2(768),\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "# print(vector_store)\n",
    "vector_store.add_documents(documents=data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilingual retriever\n",
    "multilingual_retriever = vector_store.as_retriever()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=\"Google API KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non english query\n",
    "query = \"Was ist ein Modalitätsencoder?\"\n",
    "\n",
    "# retrieve documents\n",
    "out_docs = multilingual_retriever.invoke(query)\n",
    "context = \"\\n--\\n\".join(doc.page_content for doc in out_docs)\n",
    "\n",
    "# Answer generation \n",
    "prompt_1 = PromptTemplate.from_template(\"You are a multilingual assistant. Answer user's {query} from given {context} in same language as query.\")\n",
    "chain = prompt_1 | llm\n",
    "response = chain.invoke({'query': query,\n",
    "                            'context': context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: \n",
      " Was ist ein Modalitätsencoder?\n",
      "response: \n",
      " Ein Modalitäts-Encoder ist ein Teil eines multimodalen Sprachmodells (MLLM), der rohe Informationen aus verschiedenen Modalitäten, wie Bilder, Audio oder Videos, in eine kompaktere Darstellung umwandelt. Diese Darstellung kann dann von einem großen Sprachmodell (LLM) verarbeitet werden, das normalerweise auf Textdaten trainiert wurde.\n",
      "\n",
      "**Funktionsweise:**\n",
      "\n",
      "* **Komprimierung:** Der Encoder komprimiert die rohen Informationen, z. B. ein Bild, in eine kleinere Anzahl von Merkmalen (Features).\n",
      "* **Repräsentation:** Die resultierenden Features repräsentieren die wichtigsten Informationen aus der ursprünglichen Modalität in einer Weise, die für das LLM verständlich ist.\n",
      "\n",
      "**Beispiele:**\n",
      "\n",
      "* **Bild-Encoder:** Ein Bild-Encoder könnte ein Bild in eine Reihe von Vektoren umwandeln, die die Farben, Formen und Texturen des Bildes darstellen.\n",
      "* **Audio-Encoder:** Ein Audio-Encoder könnte ein Audiosignal in eine Reihe von Vektoren umwandeln, die die Tonhöhe, Lautstärke und andere akustische Merkmale des Signals darstellen.\n",
      "\n",
      "**Vorteile:**\n",
      "\n",
      "* **Effizienz:** Durch die Komprimierung der Informationen können LLMs schneller und effizienter mit multimodalen Daten umgehen.\n",
      "* **Generalisierung:** Die Verwendung von vorab trainierten Encodern, die auf großen Datensätzen trainiert wurden, ermöglicht es LLMs, neue Daten zu generalisieren und auf verschiedene Situationen zu reagieren.\n",
      "\n",
      "**Häufig verwendete Modalitäts-Encoder:**\n",
      "\n",
      "* **CLIP:** Ein beliebter Bild-Encoder, der mit großen Mengen an Bild-Text-Paaren trainiert wurde.\n",
      "* **CLAP:** Ein Audio-Encoder, der für die Verarbeitung von Audiosignalen optimiert ist.\n",
      "* **ImageBind:** Ein Encoder, der verschiedene Modalitäten, wie Bilder, Text, Audio und Tiefeninformationen, verarbeiten kann.\n",
      "\n",
      "**Zusammenfassend:** Der Modalitäts-Encoder spielt eine wichtige Rolle bei der Integration verschiedener Modalitäten in LLMs. Durch die Komprimierung und Repräsentation von Informationen aus verschiedenen Quellen ermöglicht er LLMs, die Welt auf eine umfassendere Weise zu verstehen und mit ihr zu interagieren.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"query: \\n\",query)\n",
    "print(\"response: \\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
